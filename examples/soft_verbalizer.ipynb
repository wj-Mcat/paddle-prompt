{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Verbalzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt.data_utils.text_classification_dataset import AgnewsProcessor\n",
    "\n",
    "\n",
    "dataset = {}\n",
    "dataset['train'] = AgnewsProcessor().get_train_examples(\"./datasets/TextClassification/agnews\")\n",
    "# We sample a few examples to form the few-shot training pool\n",
    "from openprompt.data_utils.data_sampler import FewShotSampler\n",
    "sampler  = FewShotSampler(num_examples_per_label=16, num_examples_per_label_dev=16, also_sample_dev=True)\n",
    "dataset['train'], dataset['validation'] = sampler(dataset['train'])\n",
    "dataset['test'] = AgnewsProcessor().get_test_examples(\"./datasets/TextClassification/agnews\")\n",
    "\n",
    "from openprompt.plms import load_plm\n",
    "\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"t5\", \"t5-base\")\n",
    "\n",
    "# %%registered_inputflag_names\n",
    "\n",
    "from openprompt.prompts import ManualTemplate\n",
    "mytemplate = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} {\"placeholder\":\"text_b\"} In this sentence, the topic is {\"mask\"}.')\n",
    "\n",
    "\n",
    "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0]) \n",
    "print(wrapped_example)\n",
    "\n",
    "from openprompt import PromptDataLoader\n",
    "\n",
    "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer, \n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3, \n",
    "    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "\n",
    "# next(iter(train_dataloader))\n",
    "\n",
    "# ## Define the verbalizer\n",
    "# In classification, you need to define your verbalizer, which is a mapping from logits on the vocabulary to the final label probability. Let's have a look at the verbalizer details:\n",
    "\n",
    "from openprompt.prompts import SoftVerbalizer\n",
    "import torch\n",
    "\n",
    "# for example the verbalizer contains multiple label words in each class\n",
    "# myverbalizer = SoftVerbalizer(tokenizer, plm, num_classes=4,\n",
    "#          label_words=[\"politics\", \"sports\", \"business\", \"technology\"])\n",
    "# or without label words\n",
    "myverbalizer = SoftVerbalizer(tokenizer, plm, num_classes=4)\n",
    "\n",
    "\n",
    "from openprompt import PromptForClassification\n",
    "\n",
    "use_cuda = False\n",
    "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
    "if use_cuda:\n",
    "    prompt_model=  prompt_model.cuda()\n",
    "\n",
    "# ## below is standard training\n",
    "\n",
    "\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "optimizer_grouped_parameters1 = [\n",
    "    {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# Using different optimizer for prompt parameters and model parameters\n",
    "\n",
    "optimizer_grouped_parameters2 = [\n",
    "    {'params': prompt_model.verbalizer.group_parameters_1, \"lr\":3e-5},\n",
    "    {'params': prompt_model.verbalizer.group_parameters_2, \"lr\":3e-4},\n",
    "]\n",
    "\n",
    "\n",
    "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
    "optimizer2 = AdamW(optimizer_grouped_parameters2)\n",
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "    tot_loss = 0 \n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer1.step()\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.step()\n",
    "        optimizer2.zero_grad()\n",
    "        print(tot_loss/(step+1))\n",
    "    \n",
    "# ## evaluate\n",
    "\n",
    "# %%\n",
    "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer, \n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3, \n",
    "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "\n",
    "prompt_model.eval()\n",
    "\n",
    "allpreds = []\n",
    "alllabels = []\n",
    "for step, inputs in enumerate(validation_dataloader):\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = prompt_model(inputs)\n",
    "    labels = inputs['label']\n",
    "    alllabels.extend(labels.cpu().tolist())\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "print(\"validation:\",acc)\n",
    "\n",
    "\n",
    "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer, \n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3, \n",
    "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "allpreds = []\n",
    "alllabels = []\n",
    "for step, inputs in enumerate(test_dataloader):\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = prompt_model(inputs)\n",
    "    labels = inputs['label']\n",
    "    alllabels.extend(labels.cpu().tolist())\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "print(\"test:\", acc)  # roughly ~0.85\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
