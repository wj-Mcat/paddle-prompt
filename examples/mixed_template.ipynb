{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "In this tutorial example notebook, we use the glue sst-2 as the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddle_prompt.schema import InputExample\n",
    "train, dev = load_dataset(\n",
    "    'glue', 'sst-2', \n",
    "    splits=['train', 'dev']\n",
    ")\n",
    "train_examples = list(train.map(lambda x: InputExample(text_a=x['sentence'], label=x['labels'])))\n",
    "dev_examples = list(dev.map(lambda x: InputExample(text_a=x['sentence'], label=x['labels'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-04-05 18:52:14,277] [    INFO]\u001b[0m - Already cached /home/users/wujingjing/.paddlenlp/models/t5-small/spiece.model\u001b[0m\n",
      "\u001b[32m[2022-04-05 18:52:14,349] [    INFO]\u001b[0m - Already cached /home/users/wujingjing/.paddlenlp/models/t5-small/model_state.pdparams\u001b[0m\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "label template file not found, the path is: ./glue_data/tnews/manual_template.json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/mixed_template.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bailab/home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/mixed_template.ipynb#ch0000003vscode-remote?line=12'>13</a>\u001b[0m plm \u001b[39m=\u001b[39m T5ForConditionalGeneration\u001b[39m.\u001b[39mfrom_pretrained(config\u001b[39m.\u001b[39mpretrained_model)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bailab/home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/mixed_template.ipynb#ch0000003vscode-remote?line=15'>16</a>\u001b[0m label2words \u001b[39m=\u001b[39m OrderedDict({\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mnegative\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mpositive\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bailab/home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/mixed_template.ipynb#ch0000003vscode-remote?line=16'>17</a>\u001b[0m template \u001b[39m=\u001b[39m MixedTemplate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bailab/home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/mixed_template.ipynb#ch0000003vscode-remote?line=17'>18</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bailab/home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/mixed_template.ipynb#ch0000003vscode-remote?line=18'>19</a>\u001b[0m     plm\u001b[39m=\u001b[39;49mplm,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bailab/home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/mixed_template.ipynb#ch0000003vscode-remote?line=19'>20</a>\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bailab/home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/mixed_template.ipynb#ch0000003vscode-remote?line=20'>21</a>\u001b[0m     label2words\u001b[39m=\u001b[39;49mlabel2words,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bailab/home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/mixed_template.ipynb#ch0000003vscode-remote?line=21'>22</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bailab/home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/mixed_template.ipynb#ch0000003vscode-remote?line=23'>24</a>\u001b[0m verbalizer \u001b[39m=\u001b[39m ManualVerbalizer(tokenizer\u001b[39m=\u001b[39mtokenizer, label2words\u001b[39m=\u001b[39mlabel2words, config\u001b[39m=\u001b[39mconfig)\n",
      "File \u001b[0;32m~/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/mixed_template.py:40\u001b[0m, in \u001b[0;36mMixedTemplate.__init__\u001b[0;34m(self, tokenizer, config, plm, label2words, prompt_template, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/mixed_template.py?line=31'>32</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     <a href='file:///home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/mixed_template.py?line=32'>33</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     <a href='file:///home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/mixed_template.py?line=33'>34</a>\u001b[0m     tokenizer: PretrainedTokenizer, config: Config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/mixed_template.py?line=37'>38</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m     <a href='file:///home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/mixed_template.py?line=38'>39</a>\u001b[0m ):\n\u001b[0;32m---> <a href='file:///home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/mixed_template.py?line=39'>40</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(tokenizer, config, label2words\u001b[39m=\u001b[39;49mlabel2words, prompt_template\u001b[39m=\u001b[39;49mprompt_template, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/mixed_template.py?line=41'>42</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_embeddings \u001b[39m=\u001b[39m plm\u001b[39m.\u001b[39mget_input_embeddings()\n",
      "File \u001b[0;32m~/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/base_template.py:74\u001b[0m, in \u001b[0;36mTemplate.__init__\u001b[0;34m(self, tokenizer, config, label2words, prompt_template, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/base_template.py?line=71'>72</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_engine \u001b[39m=\u001b[39m JinjaEngine(prompt_template)\n\u001b[1;32m     <a href='file:///home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/base_template.py?line=72'>73</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/base_template.py?line=73'>74</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_engine \u001b[39m=\u001b[39m JinjaEngine\u001b[39m.\u001b[39;49mfrom_file(config\u001b[39m.\u001b[39;49mtemplate_file)\n\u001b[1;32m     <a href='file:///home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/base_template.py?line=75'>76</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_max_token_num()\n",
      "File \u001b[0;32m~/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/engine.py:26\u001b[0m, in \u001b[0;36mEngine.from_file\u001b[0;34m(cls, file)\u001b[0m\n\u001b[1;32m     <a href='file:///home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/engine.py?line=22'>23</a>\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     <a href='file:///home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/engine.py?line=23'>24</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_file\u001b[39m(\u001b[39mcls\u001b[39m, file: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Engine:\n\u001b[1;32m     <a href='file:///home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/engine.py?line=24'>25</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(file):\n\u001b[0;32m---> <a href='file:///home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/engine.py?line=25'>26</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlabel template file not found, the path is: \u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='file:///home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/engine.py?line=26'>27</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file_handler:\n\u001b[1;32m     <a href='file:///home/users/wujingjing/projects/bupt-nlp/paddle-prompt/examples/../src/paddle_prompt/templates/engine.py?line=27'>28</a>\u001b[0m         source_label_template \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(file_handler)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: label template file not found, the path is: ./glue_data/tnews/manual_template.json"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from paddlenlp.transformers.t5.modeling import T5ForConditionalGeneration\n",
    "from paddlenlp.transformers.t5.tokenizer import T5Tokenizer\n",
    "\n",
    "from paddle_prompt.config import Config\n",
    "from paddle_prompt.templates.mixed_template import MixedTemplate\n",
    "from paddle_prompt.verbalizers.manual_verbalizer import ManualVerbalizer\n",
    "\n",
    "config: Config = Config().parse_args(known_only=True)\n",
    "config.pretrained_model = 't5-small'\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(config.pretrained_model)\n",
    "plm = T5ForConditionalGeneration.from_pretrained(config.pretrained_model)\n",
    "\n",
    "\n",
    "label2words = OrderedDict({'0': 'negative', '1': 'positive'})\n",
    "template = MixedTemplate(\n",
    "    tokenizer=tokenizer,\n",
    "    plm=plm,\n",
    "    config=config,\n",
    "    label2words=label2words,\n",
    ")\n",
    "\n",
    "verbalizer = ManualVerbalizer(tokenizer=tokenizer, label2words=label2words, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openprompt.data_utils import InputExample\n",
    "\n",
    "dataset = {}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    dataset[split] = []\n",
    "    for data in raw_dataset[split]:\n",
    "        input_example = InputExample(text_a = data['premise'], text_b = data['hypothesis'], label=int(data['label']), guid=data['idx'])\n",
    "        dataset[split].append(input_example)\n",
    "print(dataset['train'][0])\n",
    "\n",
    "# %%\n",
    "\n",
    "from openprompt.plms import load_plm\n",
    "\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"t5\", \"t5-base\")\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# # Try more prompt!\n",
    "# You can use templates other than manual template, for example the mixedtemplate is a good place to start.\n",
    "# In MixedTemplate, you can use {\"soft\"} to denote a tunable template. More syntax and usage, please refer\n",
    "# to `How to write a template`\n",
    "from openprompt.prompts import MixedTemplate\n",
    "\n",
    "# mytemplate1 = MixedTemplate(model=plm, tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} {\"soft\": \"Question:\"} {\"placeholder\":\"text_b\"}? Is it correct? {\"mask\"}.')\n",
    "\n",
    "mytemplate = MixedTemplate(model=plm, tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} {\"soft\"} {\"soft\"} {\"soft\"} {\"placeholder\":\"text_b\"} {\"soft\"} {\"mask\"}.')\n",
    "\n",
    "\n",
    "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0]) \n",
    "print(wrapped_example)\n",
    "\n",
    "# %%\n",
    "\n",
    "wrapped_t5tokenizer = WrapperClass(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer, truncate_method=\"head\")\n",
    "\n",
    "from openprompt import PromptDataLoader\n",
    "\n",
    "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer, \n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3, \n",
    "    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "# next(iter(train_dataloader))\n",
    "\n",
    "# ## Define the verbalizer\n",
    "# In classification, you need to define your verbalizer, which is a mapping from logits on the vocabulary to the final label probability. Let's have a look at the verbalizer details:\n",
    "\n",
    "# %%\n",
    "\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "import torch\n",
    "\n",
    "# for example the verbalizer contains multiple label words in each class\n",
    "myverbalizer = ManualVerbalizer(tokenizer, num_classes=2, \n",
    "                        label_words=[[\"yes\"], [\"no\"], [\"maybe\"]])\n",
    "\n",
    "print(myverbalizer.label_words_ids)\n",
    "logits = torch.randn(2,len(tokenizer)) # creating a pseudo output from the plm\n",
    "a = myverbalizer.process_logits(logits)\n",
    "\n",
    "from openprompt import PromptForClassification\n",
    "\n",
    "use_cuda = True\n",
    "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
    "if use_cuda:\n",
    "    prompt_model=  prompt_model.cuda()\n",
    "\n",
    "# ## below is standard training\n",
    "\n",
    "\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "optimizer_grouped_parameters1 = [\n",
    "    {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# Using different optimizer for prompt parameters and model parameters\n",
    "optimizer_grouped_parameters2 = [\n",
    "    {'params': [p for n,p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n",
    "]\n",
    "\n",
    "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=1e-4)\n",
    "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    tot_loss = 0 \n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer1.step()\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.step()\n",
    "        optimizer2.zero_grad()\n",
    "        print(tot_loss/(step+1))\n",
    "    \n",
    "# ## evaluate\n",
    "\n",
    "# %%\n",
    "\n",
    "# 在预测的时候，是没有办法指定哪种template，在这里只用了一个template\n",
    "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer, \n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3, \n",
    "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "\n",
    "\n",
    "allpreds = []\n",
    "alllabels = []\n",
    "for step, inputs in enumerate(validation_dataloader):\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = prompt_model(inputs)\n",
    "    labels = inputs['label']\n",
    "    alllabels.extend(labels.cpu().tolist())\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "print(acc)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef61f8573e38e13c5c6f13e5ad7382a665dff09341ea0520089a391f1bb617cf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('paddle-latest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
